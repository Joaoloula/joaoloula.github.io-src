<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="João Loula's Blog, Collected musings on neuroscience, machine learning and math.">


        <title>Reinforcement Learning in the Brain // João Loula's Blog // Collected musings on neuroscience, machine learning and math.</title>


    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/pure/0.3.0/pure-min.css">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.1.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="http://joaoloula.github.io/theme/css/pure.css">
    <link rel="stylesheet" href="http://joaoloula.github.io/theme/css/pygments.css">

    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/fitvids/1.0.1/jquery.fitvids.min.js"></script>
    <script>
        $(document).ready(function(){
            $(".content").fitVids();
        });
    </script>
</head>

<body>
    <div class="pure-g-r" id="layout">
        <div class="sidebar pure-u">
            <div class="cover-img" style="background-image: url('/images/cover_stone.jpg')">
                <div class="cover-body">
                    <header class="header">
                        <hgroup>
                            <img class="avatar" src="/images/joao.jpg">
                            <h1 class="brand-main"><a href="http://joaoloula.github.io">João Loula's Blog</a></h1>
                            <p class="tagline">Collected musings on neuroscience, machine learning and math.</p>
                                <p class="social">
                                    <a href="https://github.com/joaoloula">
                                        <i class="fa fa-github-square fa-3x"></i>
                                    </a>
                                    <a href="https://fr.linkedin.com/in/joão-loula-2836b9107">
                                        <i class="fa fa-linkedin-square fa-3x"></i>
                                    </a>
                                    <a href="mailto:joao.campos-loula@polytechnique.edu">
                                        <i class="fa fa-envelope-square fa-3x"></i>
                                    </a>
                                    <a href="https://twitter.com/JoaoLoula">
                                        <i class="fa fa-twitter-square fa-3x"></i>
                                    </a>
                                </p>
                        </hgroup>
                    </header>
                </div>
            </div>
        </div>
    <div class="pure-u">
        <div class="content">
            <section class="post">
                <header class="post-header">
                    <h1>Reinforcement Learning in the Brain</h1>
                        <p class="post-meta">
                            // under                                 <a class="post-category" href="http://joaoloula.github.io/tag/reinforcement-learning.html">reinforcement learning</a>
                                <a class="post-category" href="http://joaoloula.github.io/tag/model-arbitration.html">model arbitration</a>
                                <a class="post-category" href="http://joaoloula.github.io/tag/episodic-memory.html">episodic memory</a>
                        </p>
                </header>
            </section>
            <h2>Introduction</h2>
<p>In this post we'll take a look at reinforcement learning, one of the most successful frameworks lately both for enabling AI to perform human-like tasks and for understanding how humans themselves learn these behaviors.</p>
<p>The premise is that of an agent in an environment in which it is trying to achieve a certain goal. The agent interacts with the environment in two ways that form a feedback loop:</p>
<ul>
<li>It receives as inputs from the environment observations and rewards</li>
<li>It outputs actions that can in their turn alter the environment</li>
</ul>
<p>It is the fact that the agent is driven to achieve a certain goal that forces it to extract, from noisy observations and uncertain rewards, a strategy for optimizing their actions. This strategy can be as simple as implementing standard responses to given stimuli and as complicated as building a sophisticated statistical model for the environment. In this post we'll take a look particularly at examples motivated by animal behavior, and discuss what the reinforcement learning framework can offer us in terms of understanding the brain.</p>
<h2>Markov Decision Processes</h2>
<p>Suppose we have an agent, say a mouse, in an environment consisting of states it can occupy, possible actions that take it from one state to another, and rewards associated with different states: for example a maze with different kinds of food scattered around it, ranging from delicious to totally unappetizing. It might look a little like this:</p>
<p align="center">
  <img src = "https://raw.githubusercontent.com/Joaoloula/joaoloula.github.io-src/master/content/posts/reinforcement-learning/maze_mouse.png"/>
</p>

<p>How can we find the path that optimizes the mouse's rewards? Well, we can start from the Bellman equation:</p>
<div class="math">$$Q\left(s_t\right) = \max_{a_t} \{ R(s_t, a_t) + Q \left(s_{t+1}\right)\}$$</div>
<p>What this equation, the principle of dynamic programming, tells us, is that calculating the Q-value of a given node is as easy as starting from the end (where the values are equivalent to the rewards) and working backwards by computing the optimal step at each time point. Following this procedure gives us the optimal path:</p>
<p align="center">
  <img src = "https://raw.githubusercontent.com/Joaoloula/joaoloula.github.io-src/master/content/posts/reinforcement-learning/maze_path_mouse.png"/>
</p>

<p>The real world, however, is a lot messier: for one thing, both state transitions and rewards are usually not deterministic, but rather probabilistic in nature. Things for our mouse might actually look more like this:</p>
<p align="center">
  <img src = "https://raw.githubusercontent.com/Joaoloula/joaoloula.github.io-src/master/content/posts/reinforcement-learning/maze_complicated_mouse.png"/>
</p>

<p>The setup is the following: the agent starts in an initial state, and at each time point he can pick one of two actions (take the arrow up or down), which can lead him to different states with some given probability. Each state is also associated with a probabilistic reward (rewards can alternatively be associated not with a state, but rather with a specific state transition) : this kind of system is what's called a Markov Decision Process -- a generalization of Markov Chains allowing for actions (i.e. control of the stochastic system) and rewards.</p>
<p>So, how can an agent go about solving this? Well, we can still take inspiration from Bellman's equation, while keeping running estimates for parameters of interest: given a policy <span class="math">\(\pi\)</span> for the agent's decision-making, an immediate reward <span class="math">\(r_t\)</span> and transition probabilities for the state space <span class="math">\(P\left(s_{t+1}| s_t, a_t\right)\)</span> at time t, we have:</p>
<div class="math">$$ Q_\pi\left(s_t, a_t\right) = r_t + \gamma\sum_{s_{t+1}} P\left(s_{t+1}| s_t, a_t\right) Q_\pi\left(s_{t+1}, \pi \left(s_{t+1}\right)\right) $$</div>
<p>We can immediately see the resemblance to the deterministic case: in fact, the second term in the right-hand side is just an expected value over the different possible transitions, seeing as the problem is now probabilistic in nature. The term <span class="math">\(\gamma\)</span> is called a discount factor, and it modulates the importance between immediate and future rewards.</p>
<p>From this equation spring the two most important reinforcement learning algorithm classes for neuroscience.</p>
<h2>Model-free learning</h2>
<p>Model-free learning focuses on estimating the left-hand side of the equation: it keeps a table of state-action pair values that is updated through experience, for example by computing a temporal difference:</p>
<div class="math">$$ \delta_t = r_t + \gamma Q (s_{t+1}, a_{t+1}) - Q(s_t, a_t) $$</div>
<p>which can then be used by a <a href="https://en.wikipedia.org/wiki/State-Action-Reward-State-Action">SARSA</a> algorithm for calculating the new state-action pair values.</p>
<p>Model-free learning pros and cons are:</p>
<ul>
<li>
<p>Computationally efficient, since decision-making consists of looking up a table.</p>
</li>
<li>
<p>Inflexible to changes in the MDP structure (transition probabilities or rewards), since they're not explicited in the model and thus can only be accounted for by relearning state-action values.</p>
</li>
</ul>
<figure>
    <img src="https://raw.githubusercontent.com/Joaoloula/joaoloula.github.io-src/master/content/posts/reinforcement-learning/dopamine.jpg" alt='missing' align='middle' />
    <figcaption> <sup> The most successful prediction of model free RL: the dopamine system. When a stimulus-reward pair is learned, dopaminergic neurons fire at the stimulus onset and not at the reward; when the reward does not succeed the stimulus, we see instead a negative firing rate. This evidence points towards the neural implementation of a TD-like algorithm <a href='#schulz' id='ref-schulz-1'>(Schultz et al., 1997)</a> </sup>
</figure>

<h2>Model-based learning</h2>
<p>Conversely, we can focus on estimating the right side of the equation: this leads to model-based learning. The idea is to keep running estimates of rewards and transition probabilities (<span class="math">\(P\left(r_{t}| s_t\right)\)</span>, <span class="math">\(P\left(s_{t+1}| s_t, a_t\right)\)</span>), and thus to have an explicit internal model for the MDP. These estimations can be computed simply by counting immediate events, and can then be strung together at decision time.</p>
<p>Model-based learning pros and cons are:</p>
<ul>
<li>
<p>Highly flexible: rewards and transition probabilities can be easily re-calculated in case of changes to the environment.</p>
</li>
<li>
<p>Computationally expensive, since running estimates of all parameters for an internal model of the MDP must be kept, and used for decision-making computations.</p>
</li>
</ul>
<h2>When to use each learning algorithm class?</h2>
<p>While from the pros and cons listed we could imagine having a grasp on which learning tasks benefit one class of algorithms over the other, a recent study <a href='#kool' id='ref-kool-1'>(Kool et al., 2016)</a> argues that the classically used Daw 2-step task <a href='#daw' id='ref-daw-1'>(Daw et al., 2011)</a> and its variations do not offer a model-free vs. model-based trade-off. It is argued that this is a cause of the following characteristics:</p>
<ul>
<li>
<p>Second-stage probabilities are not highly distinguishable</p>
</li>
<li>
<p>Drift rates for the reward probabilities are too slow</p>
</li>
<li>
<p>State transitions are non-deterministic</p>
</li>
<li>
<p>Presence of two choices in the second stage</p>
</li>
<li>
<p>Observations are binary and thus not highly informative</p>
</li>
</ul>
<p>The article goes on to propose a variant of the Daw task that addresses all these points, thus having a bigger reward range, faster drifts, deterministic transitions, no choices at the second step and continuous rewards. It goes on to show both by simulation and experiments that the trade-off is present in that variant.</p>
<h2>Integrating episodic memory and reinforcement learning</h2>
<p>Many problems arise when trying to apply the view of reinforcement learning we presented here to real-world problems solved by the brain : the following issues are of particular concern:
- State spaces are often high-dimesional and continuous, besides being only partially observable
- Observations are sparse
- The Markov property (memorylessness of the stochastic process) is not held: rewards can depend on long strings of state-action pairs.</p>
<p>How then is the brain able to learn whilst generalizing such complex structure from such limited data? A recent paper <a href='#episodic-learning' id='ref-episodic-learning-1'>(Gershman and Daw, 2016)</a> argues that episodic memory could help address these concerns.</p>
<p>Episodic memory refers to detailed autobiographical memories, things like memories of your wedding ceremony or of what you had for breakfast this morning. These instances are called <em>episodes</em>.</p>
<p>The idea of the RL model is the following: the value of a state can be approximated by the interpolation of different episodes using a kernel function <span class="math">\(K\)</span>. For example, supposing all episodes to have a fixed length <span class="math">\(t\)</span>, if we denote by <span class="math">\( s_{t}^{m}\)</span> the state at time <span class="math">\(t\)</span> under the episode <span class="math">\(m\)</span>, we have:</p>
<div class="math">$$ Q_\pi(s_0, a) = \frac{\sum_{m} R_mK(s_0, s_{t}^{m})}{N}$$</div>
<p>where <span class="math">\(R_m\)</span> is the reward for episode <span class="math">\(m\)</span>, and <span class="math">\(N\)</span> is a normalization factor, equal to <span class="math">\(\sum_m K(s_0, s_{t}^{m})\)</span>.</p>
<p>The Kernel is at the heart of the model's generalization power: it can be, for example, a gaussian allowing for smoothly combining episodes, or a step function that only averages episodes whose final states are close enough to <span class="math">\(s_0\)</span> by some distance metric. This flexibility can capture the structure of different kinds of state spaces.</p>
<p>The temporal dependency problem remains: in order to address it, we must first note that the breaking of the Markov property <em>inside</em> an episode poses no problem for the model. We can therefore chunk temporal dependencies inside episodes, using the Markov property only to stitch them together through the Bellman equation.</p>
<p>This might look something like this, by allowing episodes of various lengths <span class="math">\(t_m\)</span> and letting the Kernel take those lengths into account:</p>
<div class="math">$$ Q_\pi(s_0, a) = \frac{1}{N} \sum_{m} K(s_1, s_{t_m}^{m}, t_m) \left[R_m +\gamma^{t_m}\sum_{s}P(s_{t_m+1}^{m}=s| s_{t_m}^{m}, \pi(s_{t_m}^{m}))Q_\pi(s, \pi(s))\right]$$</div>
<p>where <span class="math">\(N\)</span> is still a normalization parameter.</p>
<h2>A link between episodes and MF/MB algorithms?</h2>
<p>It is interesting to note the influence of one of the model's parameters, namely the size of the episodes, on the learning algorithm. Take for example the Daw 2-step task, or better yet the variant proposed by <a href='#kool' id='ref-kool-2'>(Kool et al., 2016)</a>. We have two possible starting states, <span class="math">\(s_0^A\)</span> and <span class="math">\(s_0^B\)</span>, each with two possible actions <span class="math">\(a_1\)</span> and <span class="math">\(a_2\)</span> that lead deterministically to <span class="math">\(s_1\)</span> and <span class="math">\(s_2\)</span> that will, at a given trial <span class="math">\(m\)</span>, present rewards <span class="math">\(R_1^m\)</span> and <span class="math">\(R_2^m\)</span>.</p>
<p>If we denote <span class="math">\(M\)</span> the set of episodes of length 2, and <span class="math">\(M_A\)</span> and <span class="math">\(M_B\)</span> the respective subsets of episodes starting from <span class="math">\(s_0^A\)</span> and <span class="math">\(s_0^B\)</span>, the value estimations for the two possible first actions with a constant kernel will look like this:</p>
<div class="math">$$ Q_\pi(s_0^A, a_1) = \frac{1}{|M_A|} \sum_{m \in M_A} R_m^1$$</div>
<div class="math">$$ Q_\pi(s_0^B, a_1) = \frac{1}{|M_B|} \sum_{m \in M_B} R_m^1$$</div>
<p>the formulas for action 2 being analogous. We might, for example, want to add a Kernel term accounting for the recentness of the episode (to track reward drift), but I want to focus on something else for the moment: note that there is no shared term between these formulas, i.e. the value estimation for <span class="math">\(s_0^A\)</span> only looks at episodes starting in <span class="math">\(A\)</span>, and the same for <span class="math">\(s_0^B\)</span> In order words, a sudden change in the reward <span class="math">\(R_1\)</span>, if experienced during a trial starting at <span class="math">\(s_0^B\)</span>, will not influence the estimation for <span class="math">\(Q_\pi(s_0^A, a_1)\)</span> : this kind of insensitivity to reward devaluation is a trademark of model-free learning.</p>
<p>Indeed, these formulas are nothing more than value tables, and are compatible with MF learning if we imagine it being pursued with a simple reward average instead of something like a TD algorithm.</p>
<p>On the other hand, if we set the episode length to one, keeping the same notation, we'll get:</p>
<div class="math">$$ Q_\pi(s_0^A, a_1) = \frac{1}{|M_A|} \sum_{m \in M_A} R_0^A + (P(s_1|s_0^A, a_1)Q_\pi(s_1) + P(s_2|s_0^A, a_1)Q_\pi(s_2))  $$</div>
<p>,</p>
<p>but </p>
<div class="math">$$R_0^A = 0, Q_\pi(s_i)= \frac{1}{|M|} \sum_{m \in M} R_m^i$$</div>
<p>, and thus, since transitions are deterministic:</p>
<div class="math">$$ Q_\pi(s_0^A, a_1) = \frac{1}{|M|} \sum_{m \in M} R_m^1$$</div>
<div class="math">$$ Q_\pi(s_0^B, a_1) = \frac{1}{|M|} \sum_{m \in M} R_m^1$$</div>
<p>it is no surprise that, starting from episodes of length one, we recover the Bellman equation for MDPs, and finally get to an averaging version of MB learning, which presents the same value estimation for action 1, independent of whether we're in <span class="math">\(s_0^A\)</span> or <span class="math">\(s_0^B\)</span>.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%&#64;#$&#64;#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%&#64;#$&#64;#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script><hr>
<h2>Bibliography</h2>
<p id='daw'>N.D. Daw, S.J. Gershman, Seymour B., Dayan P., and Dolan&nbsp;R. J.
Model-based influences on human's choices and striatal prediction errors.
<em>Neuron</em>, 2011. <a class="cite-backref" href="#ref-daw-1" title="Jump back to reference 1">↩</a></p>
<p id='episodic-learning'>S.J. Gershman and N.D. Daw.
Reinforcement learning and episodic memory in humans and animals: an integrative framework.
<em>Annual Review of Psychology</em>, 2016. <a class="cite-backref" href="#ref-episodic-learning-1" title="Jump back to reference 1">↩</a></p>
<p id='kool'>W.&nbsp;Kool, F.A. Cushman, and S.J. Gershman.
When does model-based control pay off?
<em>PLOS Computational Biology</em>, 2016. <a class="cite-backref" href="#ref-kool-1" title="Jump back to reference 1">↩</a><a class="cite-backref" href="#ref-kool-1" title="Jump back to reference 1"> <sup>1</sup> </a><a class="cite-backref" href="#ref-kool-2" title="Jump back to reference 2"><sup>2</sup> </a></p>
<p id='schulz'>W.&nbsp;Schultz, P.&nbsp;Dayan, and P.&nbsp;R. Montague.
A neural substrate of prediction and reward.
<em>Science</em>, 1997. <a class="cite-backref" href="#ref-schulz-1" title="Jump back to reference 1">↩</a></p>

            <a href="#" class="go-top">Go Top</a>
    <div class="comments">
        <div id="disqus_thread"></div>
        <script type="text/javascript">
            /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
            var disqus_shortname = "mydisqus"; // required: replace example with your forum shortname

            /* * * DON'T EDIT BELOW THIS LINE * * */
            (function() {
                var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
                dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
                (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
            })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
        <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
    </div>
<footer class="footer">
    <p>&copy; João Loula &ndash;
        Built with <a href="https://github.com/PurePelicanTheme/pure-single">Pure Theme</a>
        for <a href="http://blog.getpelican.com/">Pelican</a>
    </p>
</footer>        </div>
    </div>
    </div>
    <script>
        var $top = $('.go-top');

        // Show or hide the sticky footer button
        $(window).scroll(function() {
            if ($(this).scrollTop() > 200) {
                $top.fadeIn(200);
            } else {
                $top.fadeOut(200);
            }
        });

        // Animate the scroll to top
        $top.click(function(event) {
            event.preventDefault();
            $('html, body').animate({scrollTop: 0}, 300);
        })

        // Makes sure that the href="#" attached to the <a> elements
        // don't scroll you back up the page.
        $('body').on('click', 'a[href="#"]', function(event) {
            event.preventDefault();
        });
    </script>
    <script type="text/javascript">
        var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
        document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
    </script>
    <script type="text/javascript">
        try {
            var pageTracker = _gat._getTracker("UA-00000000-0");
            pageTracker._trackPageview();
            } catch(err) {}
    </script>

</body>
</html>